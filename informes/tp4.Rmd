---
title: "Taller de Consultoria - TP4"
author: "Gonzalo Barrera Borla"
date: "13/10/2019"
output:
  pdf_document: default
urlcolor: blue
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
options(tinytex.verbose = TRUE)
```

# Setup
```{r, echo = T}
library(tidyverse) # manipulación de datos en general, graficos
library(broom) # limpieza y estructuracion de resultados de regresiones
library(PMCMRplus) # comparaciones múltiples para tests de rangos
library(caret) # Validación cruzada para múltiples modelos
library(gridExtra) # Paginado de gráficos 
```

# Ejercicio 1

> Se midió el daño al ADN (porcentaje de ADN que se separa durante la electroforesis) en raíces de habas sometidas a 4 tratamientos, aplicados durante diferentes tiempos. Interesa decidir si algún tratamiento es demostrablemente más dañino. Los "-" indican datos no medidos. El tiempo cero para cada uno de los tratamientos puede interpretarse como la cantidad de ADN que está inicialmente dañado, o sea que no es equivalente a "ningún tratamiento". Notar que los resultados son porcentajes, lo que implica heteroscedasticidad.

Comenzamos por analizar la relación entre media y varianza para cada tiempo y tratamiento. Como se ve a continuación, los resultados no son muy alentadores:

```{r}
tipo_cols <- cols(
  trt = col_factor(),
  t = col_integer(),
  p = col_double()
)
df1 <- read_csv("../data/4-1.csv") %>%
  rowid_to_column("id")

df1 %>%
  group_by(trt, t) %>%
  summarise_at("p", lst(mean, sd)) %>%
  arrange(desc(sd)) %>%
  knitr::kable(col.names = c("Trat.", "Tiempo", "Media", "Desvío"), digits = 3) 
```

En primer lugar, para el tratamiento IV y $t=20$, la varianza estimada es exactamente 0, por lo que la razón entre la máxima y mínima varianza para todas las combinaciones de bloque y tratamiento está indefinida, y la regla heurística que plantea Seber [1977, p.195] y utilizáramos en el TP3 no se puede uar. Aún descartando esta última observación, para el tratamiento III la varianza estimada es 12.84 en $t=20$ y sólo 0.28 en $t=30$, que arroja una altísima razón de 43.35.

Aún peor, cuando graficamos la varianza estimada en función de la media estimada, no se evidencia ninguna relación, por lo que ni siquiera sabríamos cómo morigerar la heterocedasticidad existente:

```{r}
df1 %>%
  group_by(trt, t) %>%
  summarise_at("p", lst(mean, sd)) %>%
  ggplot(aes(mean, sd, shape = factor(t), color = trt)) +
  geom_point() +
  labs(title = "Desvío estimado por tratamiento y tiempo", x = "Media",
       y = "Desvío", shape = "Tiempo", color = "Trat.")
```

Por el momento, nos bastará con haber notado esta complicación, y avanzaremos con el análisis.

¿Qué modelo puede llegar a representar adecuadamente estos datos? Para contestarnos, comenzamos graficando el porcentaje de ADN dañado en función de $t$ para cada observación, coloreada según el tratamiento. De fondo, agregamos en línea punteada el "perfil" del porcentaje promedio de ADN dañado por tratamiento y tiempo, para distinguir tendencias:

```{r}
medias1 <- df1 %>%
  group_by(t, trt) %>%
  summarise_at("p", mean)

df1 %>%
  ggplot(aes(t, p, color = trt)) +
  geom_point() +
  geom_line(data = medias1, linetype = "dashed") +
  labs(title = "Porcentaje dañado en función del tiempo", x = "Tiempo",
    y = "Porc.", color = "Trat.")
```

Pareciera que al menos el tratamiento IV se distingue desde el comienzo de los demás, y que el porcentaje de daño es siempre creciente en $t$, a una tasa constante o _tal vez_ decreciente.

Aunque esté medida en pocos valores únicos, la covariable `t` (tiempo) es cuantitativa, y por ende entendemos que es más razonable plantear lo que en la literatura se conoce como un _análisis de la covarianza_ (ANCOVA) de un factor (el tratamiento) con 4 niveles, antes que un modelo de análisis de la covarianza de 2 factores. 

Combinando estas observaciones, planteamos como modelos:

- (de referencia) un ANOVA de 2 factores,
- (y como candidatos) 6 modelos "ANCOVA" con un factor (`tratamiento`) sobre
  - polinomios de grado 1, 2 y 3 en `t`,
  - con y sin interacciones
  
Elegiremos polinomios "crudos" en lugar de los ortogonales por defecto de R para facilitar la comprensión de los coeficientes resultantes.

Si uno ajusta los modelos sobre los datos completos, no importa la métrica elegida, los modelos con más parámetros (ANOVA 2 factores y el cúbico multiplicativo) tienden a ser los que mejor ajustan: con sólo 46 datos, ajustar 16 coeficientes es casi una garantía de éxito.

Para evitar este sesgo hacia modelos más complejos, utilizamos el muy recomendable paquete `caret`, que nos permite ajustar modelos con distintas técnicas de resampleo. Para ete ejercicio, optamos por el clásico split entre datos de entrenamiento (70%) y testeo (30%), y repetimos el proceso 100 veces. A continuacion, incluimos el error cuadrático medio estimado para cada modelo, y su desvío estándar estimado, para poder seleccionar el "mejor" modelo según la ya discutida regla de un desvío estándar:

```{r, cache = TRUE}
formulas <- list(
  p ~ poly(t, 3, raw = TRUE) * trt,
  p ~ poly(t, 3, raw = TRUE) + trt,
  p ~ poly(t, 2, raw = TRUE) * trt,
  p ~ poly(t, 2, raw = TRUE) + trt,
  p ~ poly(t, 1, raw = TRUE) * trt,
  p ~ poly(t, 1, raw = TRUE) + trt,
  p ~ factor(t) * trt
)

set.seed(42)
trCont <- trainControl(method = "LGOCV", number = 100, p = 0.7)
resumen <- tibble(
  formula = formulas,
  train = map(formula, train, method = "lm", data = df1, trControl = trCont)
) %>%
  mutate(resultados = map(train, "results")) %>%
  unnest(resultados) %>%
  select(-train)

resumen %>%
  select(formula, RMSE, RMSESD) %>%
  arrange(RMSE) %>%
  knitr::kable(col.names = c("Modelo", "ECM", "Desvio"), digits = 2)
```

Como es de esperar, el modelo cúbico multiplicativo no resiste la validación cruzada. Sin embargo, el modelo que mejor ajusta, aún usando técnicas de validación cruzada, sigue siendo el ANOVA de 2 factores, que cuenta con la mayor libertad posible para adecuarse a los datos (tiene la misma cantidad de coeficientes que el cúbico multiplicativo, pero sin correlación alguna entre las covariables predictoras, que sí tienen $t, t^2, t^3$). Sin embargo, se observa que el desvío estimado para el modelo ANOVA de 2 factores es bastante alto, lo suficiente como para que la regla de 1 SD contenga cómodamente a todos los modelos salvo, justamente el cúbico multiplicativo. En este contexto, nos inclinamos por el modelo más sencillo posible, que es el lineal aditivo. Ajustamos entonces

$$
p_{ij} = \mu_i + \beta \times t + \epsilon_{ij} \quad \text{con } \epsilon_{ij} \sim N(0, \sigma^2), \: i \in \text{{I, II, III, IV}}, 1 \leq j \leq n_i
$$

donde $\mu_i$ es la ordenada correspondiente al tratamiento $i$, $t$ es el tiempo de medición y $n_i$ la cantidad de observaciones del i-ésimo tratamiento, _a sabiendas_ de que no hemos podido garantizar todavía la homocedasticidad de los $\epsilon_{ij}$. Los principales estadísticos del modelo, efectos principales y coeficientes del modelo _ajustado con todos los datos_ resultan:

```{r}
lm1 <- lm(p ~ trt + t, df1)

glance(lm1) %>%
  transmute(
    "R^2 aj." = as.character(signif(adj.r.squared, 3)),
    "F obs." = as.character(signif(statistic, 3)),
    "P-valor" = as.character(signif(p.value, 3))
  ) %>%
  knitr::kable()

tidy(anova(lm1)) %>%
  transmute(
    "Coefs" = term,
    "GL" = df,
    "C. Medios" = as.character(signif(statistic, 3)),
    "F obs." = as.character(signif(statistic, 3)),
    "P-valor" = as.character(signif(p.value, 3))
  ) %>%
  knitr::kable()

tidy(lm1) %>%
  transmute(
    "Coef." = term,
    "Estimado" = as.character(signif(estimate, 3)),
    "P-valor" = as.character(signif(p.value, 3))
  ) %>%
  knitr::kable()
```

Podemos observar que tanto (i) el modelo completo, (ii) los efectos principales del tratamiento y el tiempo y (iii) cada coeficiente individual son significativos con bajísimo p-valor. Revisamos los residuos estudentizados de las predicciones, para estimar si hemos capturado satisfactoriamente la estructura de los datos, y no observamos nada particularmente alarmante:

```{r}
df1full <- augment(lm1, data = df1)

df1full %>%
  ggplot(aes(.fitted, .std.resid, color = trt, shape = factor(t))) +
  geom_point() +
  labs(title = "Residuos estudentizadosen función del valor predicho",
       subtitle = "Ningún residuo tiene valor absoluto mayor a 2.5",
       x = "Porcentaje Predicho", y = "Residuo", color = "Trat.", shape = "Tiempo") +
  theme(legend.position = "bottom")

df1full %>%
  ggplot(aes(sample = .resid)) +
  geom_qq() +
  geom_qq_line() +
  labs(title = "QQ-plot de los residuos de predicción")

shapiro1 <- shapiro.test(df1full$.resid)
```

Lamentablemente, el gráfico cuantil-cuantil de los residuos de predicción revela que el supuesto de normalidad en los $\epsilon_{ij}$ no es realmente sostenible. Sin embargo, en este caso la distribución de los residuos se diferencia de la normal, en que tiene colas _más livianas/cortas_ que aquéfilter(df2, !loc %in% best_low)lla, en lugar de más pesadas/largas. Luego, es sostenible que esta violación del supuesto de normalidad no afecte significativamente los resultados.

A pesar de la evidente heterocedasticidad original de las observaciones, hemos obtenido un ajuste bastante satisfactorio para los datos, con una directa interpretación física. Sabemos que cuando asumimos erróneamente una matriz de covarianza $\sigma^2 I_n$ en lugar de la verdadera $V$, los estimadores de los parámetros del modelo $\hat{\beta}$ son insesgados, pero no los de menor varianza. Así y todo, hemos obtenido unos estimadores indudablemente distintos a cero, con lo cual podemos en principio confiar en ellos, ya que de usar la verdadera matriz de covarianza su significación debería mejorar.

```{r, eval = FALSE}
# NO SE EVALÚA: remoción de "outliers" y recómputo de LGOCV
plot(lm1, id.n = 10, which = 2, labels.id = df1$id)
outs <- c(2, 31, 15, 18, 46, 33, 45, 17)
lm1.2 <- lm(p ~ trt + t, df1[-outs,])
plot(lm1.2, id.n = 10, which = 2, labels.id = df1[-outs,]$id)

set.seed(42)
trCont <- trainControl(method = "LGOCV", number = 100, p = 0.7)
resumen <- tibble(
  formula = formulas,
  train = map(formula, train, method = "lm", data = df1[-outs,], trControl = trCont)
) %>%
  mutate(resultados = map(train, "results")) %>%
  unnest(resultados) %>%
  select(-train)
```

Finalmente, podemos afirmar que

- el porcentaje inicial de ADN dañado varía con el método,
- el avance del daño es aproximadamente lineal en el tiempo, y
- el avance del daño en el tiempo no varía significativamente con el método.

Los modelos ajustados son:

$$
\begin{split}
p_{I,j} &= 29.3 + 1.67 \times t + \epsilon_{I,j} \\
p_{II,j} &= 5.6 + 1.67 \times t + \epsilon_{II,j} \\
p_{III,j} &= 10.1 + 1.67 \times t + \epsilon_{III,j} \\
p_{IV,j} &= 66.5 + 1.67 \times t + \epsilon_{IV,j} \\
\end{split}
$$

Y los tratamientos, del más al menos dañino, son IV > I > III > II. Concluimos con un gráfico de las funciones ajustadas sobre los datos originales:

```{r}
grilla <- crossing(
  trt = c("I", "II", "III", "IV"),
  t = seq(0, 30, 10)
)

augment(lm1, newdata = grilla) %>%
  rename(p = .fitted) %>%
  ggplot(aes(t, p, color = trt)) +
  geom_line(linetype = "dashed") +
  geom_point(data = df1) +
    labs(title = "Porcentaje dañado predicho en función del tiempo", x = "Tiempo",
    y = "Porc.", color = "Trat.")
```

Dado que las varianzas entre diferentes poblaciones son tan heterogéneas, no podemos justificar el uso de un procedimiento de comparaciones múltiples como el de Tukey así que no presentaremos la tabla de resultados completa, pero mencionaremos que devuelve básicamente lo que uno intuitivamente observa de analizar los coeficientes y el último gráfico. Del menos al más dañino, se ordenan $II < III < I < IV$, y son todos significativamente diferentes entre sí, salvo por II y III.

# Ejercicio 2

> Se dan los rendimientos de 4 variedades de trigo en 13 localidades de Oklahoma. Interesa determinar qué variedades son  recomendables.

Si graficamos los perfiles de rendimiento en función de la localidad, con las localidades ordenadas crecientemente en rendimiento promedio para mejroar la visibilidad, vemos que las variedades de maíces se entremezclan en las localidades de menor rinde, y se separan un poco mejor en las de mayor rinde, aunque todavía con cruzamientos significativos para las variedades 1, 2 y 4:

```{r}
tipo_cols <- cols(
  loc = col_factor(),
  var = col_factor(),
  rend = col_double()
)
df2 <- read_csv("../data/4-2.csv", col_types = tipo_cols) %>%
  mutate(loc = fct_reorder(loc, rend, mean))

df2 %>%
  ggplot(aes(loc, rend, color = var, group = var)) +
  geom_point() +
  geom_line(linetype = "dashed") +
  labs(title = "Rinde promedio por variedad y localidad",
       x = "Localidad", y = "Rinde", color = "Variedad")
```

En efecto, si ajustamos un modelo del estilo $rinde \sim variedad + localidad$, tanto la localidad como las variedades parecen sumamente significativas, pero el gráfico de los residuos versus predichos tiene forma de "mariposa", lo cual nos hace sospechar de la existencia de interacciones significativas:


```{r}
lm2 <- lm(rend ~ loc + var, df2)

augment(lm2, df2) %>%
  ggplot(aes(.fitted, .std.resid)) +
  geom_point() +
  labs(title = "Residuos en función de rindes predichos para un modelo aditivo",
       x = "Rinde predicho", y = "Residuo")
```

Al tener una sola observación por celda, no podemos plantear un modelo con interacciones "completas", ya que nos quedaríamos sin grados de libertad para estimar los residuos. En su lugar, podemos plantear un modelo menos ambicioso, como el de "1 GL para la no-aditividad" de Tukey. Comenzamos por agregar una nueva covariable al modelo, $\text{int}=\hat{\alpha}_i\times \hat{\beta}_j$, donde $\alpha_i,\beta_j$ son los coeficientes qeu acompañan a la localidad $i$ y la variedad $j$, y los "sombreros" indican que los hemos estimado (como la diferencia entre la media general, y las medias por localidad y variedad, respectivamente). 

La estructura de los residuos no desaparece del todo - y aparecen algunos potenciales outliers -, pero si observamos la "tabla de ANOVA" tradicional, vemos que la inclusión del término interactivo reduce aproximadamente a la mitad la suma de cuadrados no explicada.
```{r}
df2_ <- df2 %>%
  mutate(gm = mean(rend)) %>%
  group_by(var) %>% mutate(varm = mean(rend) - gm) %>% ungroup() %>%
  group_by(loc) %>% mutate(locm = mean(rend) - gm) %>% ungroup() %>%
  mutate(int = varm*locm)

lm2_ <- lm(rend ~ loc + var + int, df2_)
lambdahat <- lm2_$coefficients["int"]
tidy(anova(lm2)) %>%
  select(term, df, sumsq) %>%
  knitr::kable(digits = 0, col.names = c("Término", "GL", "Suma Cuad."), caption = "Modelo aditivo")

tidy(anova(lm2_)) %>%
  select(term, df, sumsq) %>%
  knitr::kable(digits = 0, col.names = c("Término", "GL", "Suma Cuad."), caption = "Modelo aditivo con 1 GL para interacción")

augment(lm2_, df2_) %>%
  ggplot(aes(.fitted, .std.resid)) +
  geom_point() +
  labs(title = "Residuos en función de rindes predichos para el modelo de Tukey",
       x = "Rinde predicho", y = "Residuo")
```

A esta altura, tenemos evidencia irremontable a favor de la existencia de interacciones, lo cual resultaba intuitivamente obvio al ver el gráfico de perfiles. ¿Cómo continuamos el análisis, si todavía pretendemos dar alguna recomendación sobre qué variedades convienen en cada localidad? Usando el modelo con 1GL para las interacciones, podríamos ordenar las variedades según las predicciones en cada localidad, pero deberíamos dar una recomendación distinta en cada una, lo cual no es mucho mejor que simplemente mirar los datos crudos.

Una alternativa algo más parsimoniosa, consiste en plantear la existencia de dos subconjuntos de localidades, dentro de los cuales el modelo aditivo es sostenible, y por ende es factible dar una recomendación general. Con 13 localidades, e ignorando subconjuntos vacíos, hay $2^{12}-1=4095$ particiones posibles, con lo cual el enfoque de "probarlas todas" no resulta factible. En su lugar, vamos a considerar las 12 particiones que surgen de ordenar las localidades de menor a mayor rinde promedio, y separar las $i \in \{1,\dots,12\}$ de menor rinde por un lado (digamos, donde $rindeBajo =1$), y las $13 - i$ de mayor rinde por otro. Para cada una de las 12 particiones, ajustaremos un modelo del estilo $rinde \sim localidad + variedad \times rindeBajo$, con distintos coeficientes para cada variedad dependiendo del régimen de la localidad. Incluimos la "no-partición" con las 13 localidadades en el regímen de rinde bajo como testigo.

Para morigerar la posibilidad de sobreajuste, compararemos los 12 modelos a través de la raíz del ECM ($\widehat{RMSE}$), estimada con una estrategia de validación cruzada exigente (40 repeticiones de 10-fold CV). Los resultados obtenidos son:

```{r cache = T}
locs <- levels(df2$loc)
nlocs <- length(locs)
cortes <- list()
trCont <- trainControl(method = "repeatedcv", number = 10, repeats = 40)
f <- rend ~ (loc + var) * low
obs <- df2$rend
for (i in seq_along(locs)) {
  low_locs <- locs[1:i]
  df2_cut <- mutate(df2, low = loc %in% low_locs)
  lmObj <- lm(f, df2_cut)
  pred <- predict(lmObj)
  trainObj <- train(f, df2_cut, method = "lm", trControl = trCont)
  res <- list(
    low = paste(low_locs, collapse = ", "),
    lmObj = lmObj,
    trainObj = trainObj,
    rmse_train = caret::RMSE(pred, obs),
    rmse_test = trainObj$results$RMSE,
    rmse_test_sd = trainObj$results$RMSESD
  )
  cortes <- c(cortes, list(res))
}
```


```{r cache = T}
res <- transpose(cortes) %>%
  as_tibble() %>%
  mutate_at(c("rmse_test", "rmse_test_sd", "low", "rmse_train"), unlist) %>%
  select(-lmObj, -trainObj) %>%
  arrange(rmse_test)

knitr::kable(res, digits = 2, col.names = c("Rinde Bajo", "RMSE Train", "RMSE Test", "Desvío RMSE Test"))
```

Una vez más, los cálculos confirman la intuición. Casi cualquier partición mejora el RMSE de prueba del modelo indiviso, y la partición "óptima" considera a las localidades 12, 3, 11, 4, 9, 6 como parte de un régimen (bajo rinde, poco diferenciable) y el resto como parte de otro (alto rinde, bien diferenciables), exactamente como uno hubiese separado las localidades al ver el gráfico de perfiles original. En este punto, vale mencionar que de seguir la regla sugerida por el modelo de 1GL de Tukey, y considerar las particiones determinadas por $1 + \hat{\lambda}\times\hat{\alpha}_i > 0$, la partición hubiese resultado en $\{12, 3\}$, o si uno desea ser un poco más generoso con los márgenes y pedir que $1 + \hat{\lambda}\times\hat{\alpha}_i > 1$, el resultado sería $\{12, 3, 11, 4\}$: ninguna de las dos resulta muy buena al evaluarlas con validación cruzada.

Los residuos de este modelo aditivo en 2 grupos no evidencian estructura obvia, y el QQ-plot muestra pocos desvíos de los cuantiles teóricos. 

```{r}
best_low <- c(12, 3, 11, 4, 9, 6)
df2 <- df2 %>%
  mutate(low = loc %in% best_low)

lm2_all <- lm(rend ~ loc + var * low, df2)
lm2_low <- lm(rend ~ loc + var * low, filter(df2, loc %in% best_low))
lm2_high <- lm(rend ~ loc + var * low, filter(df2, !loc %in% best_low))

augment(lm2_all, df2) %>%
  ggplot(aes(.fitted, .std.resid, color = low)) +
  geom_point() +
  labs(title = "Residuos en función de predichos para el modelo aditivo con 2 grupos",
       x = "Rinde predicho", y = "Residuo")

augment(lm2_all, df2) %>%
  ggplot(aes(sample = .resid)) +
  geom_qq() +
  geom_qq_line() +
  labs(title = expression(paste("QQ-plot de residuos para el modelo aditivo con 2 grupos")),
       x = "", y = "") +
  theme(legend.position = "none")
```

Como hemos utilizado los mismo datos para la elección y el ajuste del modelo, no podemos hacer interpretaciones formales de los tradicionales p-valores de significatividad global, de los coeficientes, y en comparaciones múltiples, pero los presentamos a modo ilustrativo, para dar una idea intuitiva de lo conseguido. Para facilitar la comprensión, presentamos los estadísticos de significatividad global, efectos principales e interacciones para el modelo "completo", y los coeficientes de la regresión de cada regímen por separado:

```{r}
glance(lm2_all) %>%
  transmute(
    "R^2 aj." = as.character(signif(adj.r.squared, 3)),
    "F obs." = as.character(signif(statistic, 3)),
    "P-valor" = as.character(signif(p.value, 3))
  ) %>%
  knitr::kable()

tidy(anova(lm2_all)) %>%
  transmute(
    "Coefs" = term,
    "GL" = df,
    "C. Medios" = as.character(signif(statistic, 3)),
    "F obs." = as.character(signif(statistic, 3)),
    "P-valor" = as.character(signif(p.value, 3))
  ) %>%
  knitr::kable()
```


```{r}
tidy(lm2_low) %>%
  transmute(
    "Coef." = term,
    "Estimado" = as.character(signif(estimate, 3)),
    "P-valor" = as.character(signif(p.value, 3))
  ) %>%
  knitr::kable(caption = "Coeficientes para localidades de bajo rinde.")

tidy(lm2_high) %>%
  transmute(
    "Coef." = term,
    "Estimado" = as.character(signif(estimate, 3)),
    "P-valor" = as.character(signif(p.value, 3))
  ) %>%
  knitr::kable(caption = "Coeficientes para localidades de alto rinde.")
```

Para concluir, presentamos los IC simultáneos "honestos" de Tukey para diferencia de medias entre variedades, para cada regímen por separado:

```{r}
TukeyHSD(aov(lm2_low))$var %>%
  knitr::kable(digits = 3, col.names = c("Estimado", "Lím. inf. IC", "Lím. sup. IC", "P-valor"),
               caption = "ICs para diferenciasentre variedades, en localidades de bajo rinde.")
TukeyHSD(aov(lm2_high))$var %>%
    knitr::kable(digits = 3, col.names = c("Estimado", "Lím. inf. IC", "Lím. sup. IC", "P-valor"),
                 caption = "ICs para diferenciasentre variedades, en localidades de bajo rinde.")
```

Aún asumiendo que los p-valores de los intervalos de confianza simultáneos estén "inflados" por la selección de modelos, es razonable concluir que:

- en localidades de rindes bajos (12, 3, 11, 4, 9, y 6), las variedades de trigo son indistinguibles entre sí, y
- en localidades de rindes altos (2, 1, 5, 10, 8, 13 y 7), 
  - la variedad 1 es claramente mejor que todas las demás,
  - las variedades 2 y 4 son prácticamente idénticas, y
  - las variedades 2 y 4 son un algo mejores que la 3.

# Ejercicio 3

> Se desea comparar los efectos de 4 emulsiones tóxicas para el control de larvas de típula. En cada uno de 6 lotes (“bloques”) se marcaron 6 cuadrados de 90 cm. de lado; en 4 se aplicaron las emulsiones, y 2 quedaron sin tratamiento (para control). En cada bloque, la asignación se hizo al azar. Después de algunos días, se registró la cantidad de larvas sobrevivientes.  Cada cuadrado fue dividido en 9 “cuadraditos” de 30 cm. de lado, y se contaron las larvas en 2 de ellos. El objetivo es determinar cuáles emulsiones son efectivas, y si hay alguna demostrablemente mejor (para eliminar las larvas, no para engordarlas).

Para fijar el diseño experimental, necesitamos decidir (a) qué hacer con los dos tratamientos de control, y (b) cómo tratar a los dos "subbloques" (cuadraditos de 30 cm. de lado) por bloque (cuadrado de 90 cm. de lado) y tratamiento. Comenzamos por graficar los perfiles en orden creciente de larvas promedio por bloque:

```{r}
tipo_cols <- cols(
  blq = col_factor(),
  trt = col_factor(),
  larvas = col_double()
)
df3 <- read_csv("../data/4-3.csv", col_types = tipo_cols) %>%
    mutate(blq = fct_reorder(as_factor(blq), larvas, mean))

medias <- df3 %>%
  group_by(blq, trt) %>%
  summarise_all(mean)

df3 %>%
  ggplot(aes(blq, larvas, color = trt, group = trt)) +
  geom_point() +
  geom_line(data = medias, linetype = "dashed") +
  labs(title = "Larvas por bloque y tratamiento", 
       subtitle = "La línea punteada representa el promedio por celda",
       x = "Bloque", y = "Larvas", color = "Emulsión")
```

Una primera aproximación _inercial_ nos lleva a

- como no contamos con información extra sobre la existencia o no de diferencias entre los controles, y los perfiles no son muy similares, _considerar los dos controles por separado_ y
- asumiendo que la decisión de contar en sólo 2 de los 9 cuadraditos por bloque fue para ahorrar trabajo, _sumar los dos subbloques_ por celda y considerar una única observación.

Y sin hacer más supuestos sobre las distribuciones, intentar aplicar un test no paramétrico para la diferencia de medianas. Si agrupamos los subbloques y consideramos los rangos y promedios por bloque,
```{r}
df3_ <- df3 %>%
  mutate(blq = fct_reorder(as_factor(blq), larvas, mean)) %>%
  group_by(blq, trt) %>%
  summarise_all(sum)

fried3 <- friedman.test(df3_$larvas, df3_$trt, df3_$blq)

df3_ %>%
  group_by(blq) %>%
  summarise(
    delta = max(larvas) - min(larvas),
    media = mean(larvas)) %>%
  arrange(desc(delta)) %>%
  knitr::kable(col.names = c("Blq.", "Rango", "Media"), digits = 2)
```

observamos que (a) la razón del mayor al menor rango es escasamente mayor a 2 y (b) el rango por bloque aumenta junto con la media. Además, Conover [1977, p. 368] menciona que para seis o más tratamientos, el test de Friedman tiende a tener mayor potencia que Quade, circunstancias por las cuales nos inclinamos a favor del test de Friedman por sobre el de Quade. Aplicado a nuestros datos, arroja un p-valor de `r signif(fried3$p.value, 3)`, suficiente para rechazar la hipótesis nula de la igualdad de medianas, a nivel, por ejemplo, $\alpha = 0.1$. Si luego realizamos un test "post-hoc" de comparaciones múltiples significativas con el mismo $\alpha$, obtenemos:

```{r}
fried_posthoc <- PMCMRplus::frdAllPairsConoverTest(df3_$larvas, df3_$trt, df3_$blq)
fried_posthoc$p.value %>%
  knitr::kable(digits = 3)
```

Es decir, que sólo las emulsiones 3 y 4 son significativamente distintas de los controles, y a su vez ninguna de las emulsiones es distinguible de las demás. Recordando el gráfico de perfiles original, estos resultados nos resultan más bien extraños: puede que las emulsiones no se distingan claramente entre sí, pero ¿cómo puede la emulsión 3 ser significativamente distinta de ambos controles y la 2 no, si sus perfiles están básicamente superpuestos?

Pensándolo bien, creeríamos que el problema es autoimpuesto, al usar un test no-paramétrico, para comparar cantidades que están muy cerca entre sí (# de larvas para las emulsiones) contra otras que están muy lejos (los controles). Diferencias ínfimas entre emulsiones y considerables respecto al control, al pasarlas a la escala de rangos, quedan "emparejadas" en distancias unitarias, y si por azar una emulsión mata aunque sólo sea una larva menos que otra, el p-valor en sus tests de comparaciones múltiples sufrirá más de lo justificado. Para comprobar esta hipótesis, realizamos el mismo test de comparaciones múltiples de antes, pero sobre datos "sintéticos" en los cuales los tratamientos se ordenan siempre igual: 

Control 1 > Control 2 > Emulsión 1 > Emulsión 2 > Emulsión 3 > Emulsión 4. 

Los p-valores resultan:

```{r}
b <- 6
testme <- matrix(rep(6:1, b), nrow = b, ncol = 6, byrow = T)
dimnames(testme) <- list(1:b, levels(df3_$trt))
PMCMRplus::frdAllPairsConoverTest(testme)$p.value %>%
  knitr::kable(digits = 3)
```

¡Prácticamente iguales a nuestros resultados! Las dos "mejores" emulsiones (3 y 4) son significativamente distintas de los controles (la emulsión 2 resulta significativamente distinta de un sólo control), y ninguna emulsión es distinguible entre sí. En esta línea, para asegurarse que la emulsión 1 sea significativamente distinta al Control 2, ¡se requerirían 57 bloques perfectamente ordenados! Ante la evidente inadecuación del método planteado a la situación, volvemos a empezar.

Para disminuir la cantidad de comparaciones múltiples a realizar, preferimos fusionar los dos controles en un único tratamiento. Esto pasa de 15 a 10 la cantidad de comparaciones, y además nos permite estimar mejor la varianza del control y "apretar" los intervalos de confianza que lo consideren. Para hacer uso de la información cuantitativa (y no sólo la ordinal como hasta ahora) con un tradicional diseño de ANOVA con 2 factores, debemos suponer normalidad en los datos, y una misma varianza para todas las poblaciones. Comenzamos por revisar la homocedasticidad:

```{r}
df3 <- df3 %>%
  mutate(
    blq = fct_reorder(as_factor(blq), larvas, mean),
    trt = fct_collapse(trt, Control = c("Control_1", "Control_2")))

df3 %>%
  group_by(trt) %>%
  summarise_at("larvas", lst(mean, sd)) %>%
  knitr::kable(digits = 2, col.names = c("Trat.", "Media", "Desvio"))
```

Los desvíos son mas bien disímiles, con lo cual nos convendrá plantear alguna transformación estabilizadora de la varianza. Es razonable aproximar los datos con una expresión de la forma $k - X$, donde $k$ es la cantidad de larvas iniciales, y $X\sim \text{Poisson}(\lambda)$, lo que nos sugiere $g(x)=\sqrt{x}$ como transformación estabilizadora. Despues de esta operación, la tasa del mayor (Control, 1.27) al menor (Emulsión 2, 0.74) desvío resulta ser $\approx 1.72$, lo cual nos parece tolerable. Graficamos los perfiles de los datos transformados, junto con los QQ-plot por tratamiento:

```{r}
df3 <- df3 %>%
  mutate(larvas_ = sqrt(larvas))

df3 %>%
  group_by(trt) %>%
  summarise_at("larvas_", lst(mean, sd)) %>%
  knitr::kable(digits = 2, col.names = c("Trat.", "Media", "Desvio"))
```


```{r}
medias <- df3 %>%
  group_by(blq, trt) %>%
  summarise_at("larvas_", mean)

medias %>%
  ggplot(aes(blq, larvas_, color = trt, group = trt)) +
  geom_point() +
  geom_line(data = medias, linetype = "dashed") +
  labs(title = expression(paste("Promedio de ", sqrt(larvas), "  por tratamiento y bloque")),
       x = "Bloque", y = expression(sqrt(larvas)), color = "Tratamiento")
```

```{r}
df3 %>%
  ggplot(aes(sample = larvas_, color = trt)) +
  geom_qq() +
  geom_qq_line() +
  facet_wrap(~ trt, scale = "free_y") +
  labs(title = expression(paste("QQ-plot de ", sqrt(larvas), "  por tratamiento")),
       x = "", y = "") +
  theme(legend.position = "none")
```

Salvo por una observación atípica para la emulsión 4, los datos correspondientes a las 4 emulsiones tiene una distribución aceptablemente similiar a una normal. Para los controles combinados, una distribución normal no parece demasiado adecuada en los extremos, pero tampoco es aberrante, así que por el momento consideramos que los supuestos de normalidad y homocedasticidad se sostienen razonablemente. Ajustamos un modelo de efectos aditivos del estilo $\sqrt{larvas} \sim bloque + emulsión$, y examinamos los residuos de las predicciones:

```{r}
lm3 <- lm(larvas_ ~ blq + trt, df3)
df3aug <- augment(lm3, data = df3)

df3aug %>%
  ggplot(aes(.fitted, .std.resid, color = trt)) +
  geom_point() +
  labs(title = "Residuos en función de predichos",
       x = "Predicho", y = "Residuo", color = "Emulsión")

df3aug %>%
  ggplot(aes(sample = .resid)) +
  geom_qq() +
  geom_qq_line() +
  labs(title = "QQ-plot de residuos de predicción", x = "", y = "")
```

No nos encontramos con ninguna estructura obvia, pero sí observamos un par de _outliers_ con residuo mayor a 3 correspondientes a las observaciones con 84 y 71 larvas del Control 1 en los bloques 6 y 4, respectivamente. Esto era de esperar, ya que como vimos en los QQ-plots por población, el control se alejaba de la normalidad en los extremos. Gracias a haber fusionado los dos controles en uno sólo, podemos descartar ambas observaciones sin dejar de tener un estimador de la varianza para en control en dichos bloques, así que las abandonamos. El ajuste resultante luce fundamentalmente creíble:

```{r}
df3 <- filter(df3, larvas < 70)
lm3 <- lm(larvas_ ~ blq + trt, df3)
df3aug <- augment(lm3, data = df3)

df3aug %>%
  ggplot(aes(.fitted, .std.resid, color = trt)) +
  geom_point() +
  labs(title = "Residuos en función de predichos",
       x = "Predicho", y = "Residuo", color = "Emulsión")

df3aug %>%
  ggplot(aes(sample = .resid)) +
  geom_qq() +
  geom_qq_line() +
  labs(title = "QQ-plot de residuos de predicción", x = "", y = "")
```

```{r}
glance(lm3) %>%
  transmute(
    "R^2 aj." = as.character(signif(adj.r.squared, 3)),
    "F obs." = as.character(signif(statistic, 3)),
    "P-valor" = as.character(signif(p.value, 3))
  ) %>%
  knitr::kable()

tidy(anova(lm3)) %>%
  transmute(
    "Coefs" = term,
    "GL" = df,
    "C. Medios" = as.character(signif(statistic, 3)),
    "F obs." = as.character(signif(statistic, 3)),
    "P-valor" = as.character(signif(p.value, 3))
  ) %>%
  knitr::kable()

tidy(lm3) %>%
  transmute(
    "Coef." = term,
    "Estimado" = as.character(signif(estimate, 3)),
    "P-valor" = as.character(signif(p.value, 3))
  ) %>%
  knitr::kable()
```

Se observa inmediatamente que el modelo produce un buen ajuste, y que el efecto de los tratamientos es indudablemente significativo, tanto conjuntamente como individualmente. El efecto de los bloques es significativo al tradicional nivel del 5%, por lo que hemos decidido conservarlo en el modelo, pero obviar los bloques podría ser una decisión razonable también para mejorar la interpretabilidad de los resultados.

Luego de fusionar los dos controles y eliminar los valores extremos de larvas, los desvíos estimados por tratamiento son fundamentalmente idénticos, lo cual nos permitirá aplicar un test de comparaciones múltiples de Tukey con confianza:

```{r}
df3 %>%
  group_by(trt) %>%
  summarise_at("larvas_", lst(mean, sd)) %>%
  knitr::kable(digits = 2, col.names = c("Trat.", "Media", "Desvio"))
```


```{r}
TukeyHSD(aov(lm3), conf.level = 0.05)$trt %>%
  knitr::kable(digits = 3, col.names = c("Estimado", "Lím. inf. IC", "Lím. sup. IC", "P-valor"))
```

A nivel 5%, podemos concluir que

- el tratamiento con cualquier emulsión es efectivo para eliminar larvas,
- la emulsión 1 es significativamete más efectiva que las 3 y 4, pero no que la 2, y
- la emulsión 2 es significativamnte más efectiva que 4, pero no que la 3, y
- las emulsiones 3 y 4 no se distinguen entre sí.

Mas allá de la significatividad estadística, de tener que decantarse por una única opción, nuestra sugerencia sería usar la emulsión 1.
