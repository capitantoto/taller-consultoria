---
title: "Taller de Consultoria - TP2 - Selección de Modelos"
author: "Gonzalo Barrera Borla"
date: "11/10/2019"
output:
  pdf_document: default
urlcolor: blue
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
options(tinytex.verbose = TRUE)
library(tidyverse)
library(broom)
library(GGally)
library(caret)
```

## Introducción

En los tres ejercicios de esta práctica se busca

- (a) elegir un modelo para predecir la variable $y$ a partir de una muestra aleatoria $X$ de un conjunto de covariables $x_1, x_2, \dots, x_n$, y
- (b) dar una medida del error de predicción asociado al modelo.

A diferencia del TP1, los tres problemas que ahora nos competen ilustran situaciones tan específicas que comprender la relación entre las covariables sin la ayuda de un técnico en el campo es una tarea vana. La _opacidad_ del conjunto de covariables (que no es una caja negra, pero tampoco es transparente) nos obliga a intentar ajustar varios modelos en cada situación, y elegir el "mejor" entre ellos, según algún criterio de bondad a definir. Luego, en cada ejercicio intentaremos encontrar el "mejor modelo lineal", siguiendo el mismo procedimiento:

1. **Detección de Outliers**: asumiendo que los datos tiene distribución normal multivariada, la distancia de Mahalanobis a la media poblacional de cada observación $(\mathrm{x}_i-\mu)' \Sigma^{-1} (\mathrm{x}_i-\mu) \sim \chi^2_p$. Computaremos un estimador robusto de la dispersión y locación muestral usando `RobStatTM::covRob` y en un QQ-plot veremos cuáles de las distancias de Mahalanobis Robustas se alejan significativamente de una distribución $\chi^2_p$. Dichas observaciones serán consideradas _outliers_, y se repetirá iterativamente este mismo análisis dejándolas afuera, hasta no encontrar más outliers.

2. **Selección de Modelos**: Con el dataset despojado de outliers, compararemos varios modelos posibles entre sí, utilizando como criterio de bondad el ECM de predicción por "validación cruzada deje-uno-fuera" (LOOCV, por sus siglas en inglés). Para evitar el sobreajuste a los datos, utilizaremos la empíricamente eficiente "Regla de 1 Desvío Estándar". Cconsideraremos que el "mejor modelo" no es el que tenga el menor ECM, sino aquél de menor complejidad (_id est_, con menor cantidad de regresores) con un ECM no más de 1DE por encima del mínimo ECM en términos absolutos.

3. **Análisis de residuos**: Elegido el mejor modelo según la R1DE, revisaremos los residuos de predicción para comprobar que (a) no posean ninguna estructura obvia y (b) sean aproximadamente normales. De violarse severamente alguno de estos supuestos, se lo descartará y volveremos a la etapa de selección de modelos.

Armados de un criterio sólido de selección de modelos, a continuación hacemos sólo una breve sinopsis de los resultados obtenidos aplicando el mismo criterio a cada problema sin detenernos demasiado en ninguno. El lector avieso notará que los conjuntos de modelos sobre los cuales elegiremos "el mejor" o "el más parsimonioso" distan de ser exhaustivos. Esto es adrede, ya que cualquier enumeración completa de los modelos es una quimera, y estamos realizando este ejercicio casi a modo ilustrativo. En la vida real, sería irresponsable ofrecer un modelo así elegido, sin entender a fondo las covariables involucradas.

Por último, unas palabras sobre la precisión de la predicción. Al usar LOOCV como método de validación, es natural reportar la raíz del ECM como medida de precisión, que está en la misma escala que la variable a predecir $y$.Sin embargo, asumir que estas medidas son sinónimos con el verdadero error de predicción sería un error, y la justificación de su uso es más bien intuitiva y por comodidad.

```{r metodos}
loocv <- function(formula, data) {
  n <- nrow(data)
  preds <- vector("double", n)
  for (i in seq.int(n)) {
    lm_fit <- lm(formula, data[-i,])
    preds[i] <- predict(lm_fit, newdata = data[i,])
  }
  return(preds)
}

sqdif <- function(x, y) { (x-y)^2 }
resumir <- function(data, ycol, modelos) {
  y <- data[[ycol]]
  enframe(modelos, name = "nombre", value = "formula") %>%
  mutate(
    loocv_preds = map(formula, loocv, data),
    sqerrs = map(loocv_preds, sqdif, y),
    msehat = map_dbl(sqerrs, mean),
    msehatvar = map_dbl(sqerrs, var))
}

imprimir_resumen <- function(res) {
  res %>%
  transmute(
    modelo = map_chr(formula, deparse),
    ec = signif(msehat, 4),
    desvio = signif(sqrt(msehatvar), 4))%>%
  arrange(ec) %>%
  knitr::kable(
    col.names = c("Modelo", "ECM", "Desvío")
  )
}
```

## Ejercicio 1

La definición de las covariables en el ejercicio tiene algunas particularidades.  Por un lado, la "gravedad" API [2] es una escala de densidad específica, propuesta por el American Petroleum Institute. Según las clasificaciones tradicionales, todos los tipos de petróleo evaluados tienen una gravedad mayor a 31.1° (i.e., menor a 870 kg/m3) y por ende son "crudos ligeros". Las otras dos variables con curiosa escala, A y V, están expresadas en "punto ASTM". La "Sociedad Americana para Pruebas y Materiales", o ASTM por sus siglas en inglés, tiene publicados más de 12.000 (!) estándares a la fecha, así que saber a cuál hace referencia la descripción es como buscar una aguja en un pajar. Si fuésemos hombres de apuestas, seguramente nos jugaríamos por el [3] _ASTM D1837-17: Standard Test Method for Volatility of Liquefied Petroleum (LP) Gases_. Lamentablemente, el estándar fue retirado de circulación en 2017 al haber sido supeditado por métodos más directos y eficientes como la cromatografía de gases.

Una primera dificultad al analizar los datos surge al notar que los pares de covariables $(tipo, P), (tipo, A), (tipo, G)$ son perfectamente colineales. Esto es razonable, ya que $P, G, A$ son _características_ de un cierto $tipo$ de combustible, pero conducen a matrices de rango incompleto al incluir a más de una de ellas en un modelo.
El único tratamiento que se le dio a los datos, entonces, fue 

- eliminar la variable $corrida$: el orden de fabricación de cada lote no debería afectar el resultado. Además, hay una fuerte e inexplicable correlación entre la volatilidad final $V$ y la corrida para todos los tipos de crudo, que no conviene incluir.
- transformar la variable $tipo$ en factor: si usásemos el valor entero que se provee, estaríamos asumiendo implícitamente un orden entre los distintos tipos de crudo, que la información a mano no justifica. Es preferible tratarla como categórica, y dejar que R genera las "dummies" que hagan falta para representarla.

A continuación, presentamos en forma de tabla las tuplas $(\omega_j,\:\hat{\mu}_j,\:\hat{\sigma}_j, \rho_j)$ para una selección arbitraria de modelos posibles, con 4 cifras significativas:
```{r ej1}
tipo_cols <-  cols(
  tipo = col_factor(),
  corrida = col_integer(),
  G = col_double(),
  P = col_double(),
  A = col_double(),
  V = col_integer(),
  R = col_double()
)
df1 <- read_csv("../data/2-1.csv", col_types = tipo_cols)

modelos1 <- list(
  lineal_completo = R ~ corrida + tipo + A + P + G + V,
  lineal_sin_corrida = R ~ tipo + A + P + G + V,
  APGV = R ~ A + P + G + V,
  A = R ~ A,
  P = R ~ P,
  G = R ~ G,
  V = R ~ V,
  AP = R ~ A + P,
  AG = R ~ A + G,
  AV = R ~ A + V,
  PG = R ~ P + G,
  PV = R ~ P + V,
  GV = R ~ G + V,
  APG = R ~ A + P + G,
  APV = R ~ A + P + V,
  AGV = R ~ A + G + V,
  PGV = R ~ P + G + V,
  cuadratico_completo = R ~ (tipo + A + P + G + V)^2,
  V_por_tipo = R ~ V*tipo,
  V_mas_tipo = R ~ V + tipo
)

res1 <- resumir(df1, "R", modelos1)
imprimir_resumen(res1)

set.seed(42)
trCont1 <- trainControl(method = "LGOCV", number = 100, p = 0.7)
resumen1 <- tibble(
  formula = modelos1,
  train = map(formula, train, method = "lm", data = df1, trControl = trCont)
) %>%
  mutate(resultados = map(train, "results")) %>%
  unnest(resultados) %>%
  select(-train)
```

Resulta entonce que $\omega^{ep}:= R \sim tipo + A + P + G + V$, con complejidad $c_{ep}= 6$ (si consideramos a las 9 dummies de $tipo$ como una sola) o $c_{ep}=14$ (si somos estrictos con la definición, pero dentro de un desvío estándar de $\hat{\mu}_{ep}$ encontramos $\omega^{\star} := R \sim A + V$, con prácticamente la misma precisión (~16%), un error $\hat{\mu}_{\star}$ muy similar, pero mucha menor complejidad, $c_{\star}=3$.

Para asegurarnos de que el modelo no sólo ajuste bien sino que además esté libre de sesgos sistemáticos, graficamos los residuos de las predicciones de LOOCV contra los valores reales. Como referencia, incluimos líneas horizontales en $(-2.5, +2.5)$ desvíos estándar de los residuos muestrales. No se observa ninguna estructura evidente, así que confirmamos la selección previa.
```{r ej1_residuos}
res1 %>%
  select(nombre, loocv_preds) %>%
  filter(nombre == "AV") %>%
  mutate(y = list(df1$R)) %>%
  unnest() %>%
  mutate(r = y - loocv_preds) %>%
  group_by(nombre) %>%
  mutate(desvio = sd(r)) %>%
  ggplot(aes(y, r)) +
  geom_point() +
  geom_hline(mapping = aes(yintercept = -2.5*desvio), alpha=0.5) +
  geom_hline(mapping = aes(yintercept = 0), alpha=0.5) +
  geom_hline(mapping = aes(yintercept = 2.5*desvio), alpha=0.5)
```

## Ejercicio 2

Si las variables del ejercicio 1 estaban curiosamente definidas, estas dan aún más lugar para la imaginación: un "flujo de aire" $X_1$ en unidades desconocidas, una concentración de ácido $X_3$ en porcentaje (¿de qué? ¿de los reactivos totales? ¿por qué no se usará el pH?), y una pérdida también en unidades desconocidas.

Como especifica el ejercicio, excluimos la variable $dia$ de (casi) todos los modelos a comparar, con un par de excepciones por curiosidad intelectual. Presentamos análogo resumen al del ejercicio anterior:
```{r ej2}
tipo_cols <- cols(
  dia = col_integer(),
  X1 = col_integer(),
  X2 = col_integer(),
  X3 = col_double(),
  Y = col_double()
)
df2 <- read_csv("../data/2-2.csv", col_types = tipo_cols)

modelos2 <- list(
  completo = Y ~ dia + X1 + X2 + X3,
  X1X2 = Y ~ X1 + X2,
  X1X3 = Y ~ X1 + X3,
  X2X3 = Y ~ X2 + X3,
  X1yX2 = Y ~ X1:X2,
  X1 = Y ~ X1,
  X2 = Y ~ X2,
  X3 = Y ~ X3,
#  cte = Y ~ 1,
  sindia = Y ~ X1 + X2 + X3,
  cuadratico = Y ~ (X1 + X2 + X3)^2,
  cubico = Y ~ (X1 + X2 + X3)^3
)

res2 <- resumir(df2_, "Y", modelos2)
imprimir_resumen(res2)
trCont2 <- trainControl(method = "LGOCV", number = 20, p = 0.7)
set.seed(42)
resumen2 <- tibble(
  formula = modelos2,
  train = map(formula, train, method = "lm", data = df2, trControl = trCont2)
) %>%
  mutate(resultados = map(train, "results")) %>%
  unnest(resultados) %>%
  select(-train)

set.seed(42)
resumen2_ <- tibble(
  formula = modelos2,
  train = map(formula, train, method = "lm", data = df2_, trControl = trCont2)
) %>%
  mutate(resultados = map(train, "results")) %>%
  unnest(resultados) %>%
  select(-train)

trCont2 <- trainControl(method = "LOOCV")
set.seed(42)
resumen2_t <- tibble(
  formula = modelos2,
  train = map(formula, train, method = "lm", data = df2_, trControl = trCont2)
) %>%
  mutate(resultados = map(train, "results")) %>%
  unnest(resultados) %>%
  select(-train)

```

Curiosamente, $\omega_{ep} := Y \sim dia + X_1 + X_2 + X_3$, con un $\hat{\mu}$ ligeramente mejor que el mismo modelo, sin día. En otras palabras, pareciera ser que _algo_ de información está codificada en esa secuencia, pero incluirla en el modelo sabiendo tan poco es un tanto irresponsable.
En este caso, el desvío estimado es tan alto para $\omega_{ep}$ que no sólo uno, sino dos modelos de una sola covariable entran en el intervalo de la R1DE: $Y \sim X_1$ y $Y \sim X_2$. En principio elegiremos $\omega_{\star} := Y \sim X_1$ que es (por mucho) el de menor ECM estimado entre ambos. Otro dato interesante, es que aunque $5 = c_{ep} > c_{\star} = 2$, $\rho_{\star} \approx 0.17 < 0.19 \approx \rho_{ep}$: ¡la precisión del modelo más simple, asi medida, es mayor que la del modelo más complejo! Esto se debe, probablemente, a que alguna predicción LOOCV de $\omega_{\star}$ haya sido particularmente mala, lo cual lo penaliza relativamente más en nuestro estimador del ECM $\hat{\mu}$, que en nuestra medida de precisión $\rho$.

Al igual que antes, graficamos los residuos contra los valores reales para comprobar la existencia de estructura en ellos. Lamentablemente, resulta claro que _algo_ extraño hay en los residuos, con al menos un candidato a _outlier_ por fuera de la banda de $\pm2.5SD$, y una intrigante estructura en "zig zag" en los primeros datos.

```{r ej2_residuos}
res2 %>%
  select(nombre, loocv_preds) %>%
  filter(nombre == "X1") %>%
  mutate(y = list(df2_$Y)) %>%
  unnest() %>%
  mutate(r = y - loocv_preds) %>%
  group_by(nombre) %>%
  mutate(desvio = sd(r)) %>%
  ggplot(aes(loocv_preds, r)) +
  geom_point() +
  geom_hline(mapping = aes(yintercept = -2.5*desvio), alpha=0.5) +
  geom_hline(mapping = aes(yintercept = 0), alpha=0.5) +
  geom_hline(mapping = aes(yintercept = 2.5*desvio), alpha=0.5)
res2 %>%
  select(nombre, loocv_preds) %>%
  filter(nombre == "X1") %>%
  mutate(y = list(df2_$Y)) %>%
  unnest() %>%
  mutate(r = y - loocv_preds) %>%
  ggplot(aes(sample = r)) +
  geom_qq() +
  geom_qq_line()

```

Para entender un poco mejor qué estamos viendo, recurrimos a `GGally::ggpairs`, que realiza una cómoda visualización del los _scatter plots_ de cada par de variables de interés, en este caso, $(X_1, X_2, X_3, Y)$:
```{r}
ggpairs(df2, columns = 2:5)
```

De aquí, aprendemos dos cosas importantes que explican el gráfico anterior. Primero, que efectivamente hay una observación muy por fuera de la tendencia lineal entre $(X_1, Y)$, y por ello el _outlier_ de antes. Y en segundo lugar, se observan muchos valores repetidos de $X_1$, para distintas $Y$, que en el modelo $\omega_{\star}:=Y \sim X_1$ darían lugar a predicciones repetidas, lo cual explica las "escaleritas".

De los scatter plots previos, pareciera ser el caso que el _outlier_ observado tiene tal carácter por los valores conjuntos de $(X_1, Y)$, y no del resto de las covariables. Para ver si este es el caso, a continuación graficamos los residuos de LOOCV, para _todos_ los modelos considerados:
```{r, fig.height=12, fig.width=8}
res2 %>%
  transmute(
    nombre = map_chr(formula, deparse),
    loocv_preds) %>%
  mutate(y = list(df2_$Y)) %>%
  unnest() %>%
  mutate(r = y - loocv_preds) %>%
  group_by(nombre) %>%
  mutate(desvio = sd(r)) %>%
  ggplot(aes(loocv_preds, r)) +
  geom_point() +
  geom_hline(mapping = aes(yintercept = -2.5*desvio), alpha=0.5) +
  geom_hline(mapping = aes(yintercept = 0), alpha=0.5) +
  geom_hline(mapping = aes(yintercept = 2.5*desvio), alpha=0.5) +
  facet_wrap(~nombre,ncol = 3)
```

Efectivamente, vemos que cada vez que incluimos a $X_1$ en un modelo, nos topamos con exactamente una predicción descabellada, y si no la incluimos, no se observa tal outlier. Aquí, dos caminos divergen en el "bosque" del análisis [7]. O bien

- consideramos efectivamente como _outlier_ a la observación en duda, rehacemos el análisis sin ella, el ECM de todos los modelos que incluyan a $X_1$ mejora, y casi seguramente elijamos los mismos $\omega_{ep}, \: \omega_{\star}$ que antes, o
- **no** consideramos _outlier_ al dato, lo mantenemos dentro del _dataset_ de entrenamiento, y corregimos la selección de modelos para asegurarnos que los residuos del modelo elegido no presenten estructura obvia.

En la vida real, nos acercaríamos al técnico o analista que nos haya provisto los datos y le preguntaríamos por la observación anómala, para saber si conservarla tal cual, corregirla o descartarla. Como no podemos darnos ese lujo, debemos tomar una decisión "a ciegas", y siguiendo un critero de "observación inocente (válida) hasta que se pruebe lo contrario", preferimos conservarla y **cambiar la selección de modelo a** $\omega_{\star^{\prime}}:=Y \sim X_2$, que cumple la condición de parsimonia (R1DE) y no presenta mayores particularidades en la estructura de los residuos.

Por último, y anecdóticamente, nótese que los modelos cuadrático ($c_{2}=10$) y cúbico ($c_{3}=20$, si no me equivoco) en todas las covariables no agregan nada de información, y por ende no mejoran el ECM. Tal vez, si contásemos con miles y miles de observaciones de las cuales extraer estructura y las mismas únicas 3 covariables, la historia sería distinta.

## Ejercicio 3

Investigando en la Internet, descubrimos que efectivamente, la espectroscopía del espectro infrarrojo cercano (NIR, por sus siglas en inglés) se utiliza en agricultura [4] para medir la calidad de distintos cultivos y suelos, ya que es un método no-invasivo y relativamente barato. El "estándar dorado", sin embargo, para medir el contenido de nitrógeno en sustancias orgnánicas, e indirectamente el contenido proteico de las mismas, es el método de Kjeldahl [5]. Vale aclarar que ambos métodos son formas indirectas de medir la cantidad de proteína en una muestra, y por lo tanto son susceptibles a manipulaciones. En un incidente particularmente cruento [6], en 2008 más de 54,000 bebés fueron hospitalizados y al menos 12 murieron por cálculos renales o insuficiencia proteica cuando varias marcas de leche en polvo adulteraron sus productos con melamina, un químico compuesto en 67% m/m por nitrógeno, más conocido como la materia prima para los revestimientos de fórmica. Como bien hemos aprendido ya, hacer inferencia sobre cierto soporte de los datos y luego aplicar los resultados obtenidos a muestras por fuera de ese dominio, no garantiza resultados válidos. 

Siendo ésta data de _calibración_ de un instrumento, asumimos que las muestras de trigo no están adulteradas, pero tampoco tenemos mucha información sobre la cual trabajar, ya que ni siquiera conocemos las longitudes de onda medidas, como para poder estimar un perfil de absorción de radiación. Así las cosas, con 6 covariables disponibles, sólo los posibles modelos lineales en ellas ascienden a $2^6=64$, de modo que _tras bambalinas_ primero recurrimos al método `GGally::ggpairs` para graficar los "scatterplots" y calcular la correlación entre cada par de variables disponibles, pero no obtuvimos demasiada información: las seis variables $L_i$ están tan correlacionadas que todos los scatterplots se ven iguales: una línea recta con pendiente positiva.
A continuación, sacamos la artillería pesada, y usamos el método `olsrr::ols_all_subset` para ajustar todos los modelos lineales posibles, elegir un subconjunto de los más prometedores, y calcular el ECM según LOOCV como hasta ahora. 

```{r ej3}
tipo_cols <- cols(
  muestra = col_integer(),
  Y = col_double(),
  L1 = col_integer(),
  L2 = col_integer(),
  L3 = col_integer(),
  L4 = col_integer(),
  L5 = col_integer(),
  L6 = col_integer()
)
df3 <- read_csv("../data/2-3.csv", col_types = tipo_cols)

modelos3 <- list(
  completo = Y ~ muestra + L1 + L2 + L3 + L4 + L5 + L6,
  sinmuestra = Y ~ L1 + L2 + L3 + L4 + L5 + L6,
  cuadrado = Y ~ (L1 + L2 + L3 + L4 + L5 + L6)^2,
  L3456 = Y ~ L3 + L4 + L5 + L6,
  L345 = Y ~ L3 + L4 + L5,
  L34 = Y ~  L3 + L4,
  L34_cuadrado = Y ~ (L3 + L4)^2,
  L3 = Y ~ L3,
  L4 = Y ~ L4
)

res3 <- resumir(df3, "Y", modelos3)
imprimir_resumen(res3)
```

$\omega_{ep} := Y \sim L_3 + L_4 + L_5$, con $c_{ep}=4$, y $\omega_{\star} := Y \sim L_3 + L_4$, con $c_{\star}=3$. La precisión de ambos modelos es muy similar y dado lo indirecto del método, sorprendentemente buena: ligeramente por encima del 2%, comparado con el ~16% de los ejercicios previos.

Como siempre, graficamos los residuos para estudiar cualquier estructura remanente.

```{r ej3_residuos}
res3 %>%
  filter(nombre=="L34") %>%
  transmute(
    nombre = map_chr(formula, deparse),
    loocv_preds) %>%
  mutate(y = list(df3$Y)) %>%
  unnest() %>%
  mutate(r = y - loocv_preds) %>%
  group_by(nombre) %>%
  mutate(desvio = sd(r)) %>%
  ggplot(aes(y, r)) +
  geom_point() +
  geom_hline(mapping = aes(yintercept = -2.5*desvio), alpha=0.5) +
  geom_hline(mapping = aes(yintercept = 0), alpha=0.5) +
  geom_hline(mapping = aes(yintercept = 2.5*desvio), alpha=0.5) +
  facet_wrap(~nombre,ncol = 3)
```

En principio, no observamos una tendencia obvia en los residuos. Podríamos llegar a sospechar cierta heterocedasticidad, con menor varianza a medida que $Y$ aumenta, pero la cantidad de observaciones disponibles es más bien pobre para concluir al respecto. En principio, confirmamos la selección de modelo, dejando un "asterisco" en el supuesto de homocedasticidad para testear su validez en caso de obtener más datos.

Como en los ejercicios previos, si trazásemos un intervalo de confianza alrededor del estimador puntual, $0 \in (\hat{\mu}_{ep} \pm 1\hat{\sigma}_{ep})$. Evidentemente, (y esto es natural considerando que el ECM es siempre positivo), la distribución no es simétrica alrededor de la media, y ofrecer un verdadero IC para el ECM no es ejercicio trivial. En la misma línea,  tal vez a causa del pequeño tamaño muestral  $\hat{\mu}_{ep} < \hat{\sigma}_{ep}$, y la regla de 1SD deja verdaderamente mucho espacio para elegir modelos parsimoniosos. En este caso, resulta ilustrativo ver que por separado $Y \sim L_3$ y $Y \sim  L_4$ son modelos con enorme error, pero el modelo combinado es ~6 veces más preciso y con ~25 veces menor ECM.

Por último, se ve que con variables tan correlacionadas, agregar los términos cuadráticos al modelo dispara el ECM de VC al demonio (aunque de seguro no puede disparar el ECM de entrenamiento), cosa que no necesariamente sucede cuando hay menos términos disponibles como en el ejercicio 2. 

## Referencias

- [1] Justificacion empírica de la "regla de un desvío estándar" - [enlace](https://stats.stackexchange.com/questions/80268/empirical-justification-for-the-one-standard-error-rule-when-using-cross-validat)
- [2] Gravedad API - [enlace](https://en.wikipedia.org/wiki/API_gravity)
- [3] ASTM D1837-17: Standard Test Method for Volatility of Liquefied Petroleum (LP) Gases (Withdrawn 2017) - [enlace](https://www.astm.org/Standards/D1837.htm)
- [4] Espectroscopía infrarroja cercana (NIR) en agricultura - [enlace](https://en.wikipedia.org/wiki/Near-infrared_spectroscopy#Agriculture)
- [5] Método Kjeldahl - [enlace](https://en.wikipedia.org/wiki/Kjeldahl_method)
- [6] Adulteración de leche para bebés en 2008 - [enlace](https://en.wikipedia.org/wiki/2008_Chinese_milk_scandal)
- [7] "The Road Not Taken", de Robert Forst - [enlace](https://www.poetryfoundation.org/poems/44272/the-road-not-taken)